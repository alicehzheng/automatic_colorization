{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "                   ab_max: 110.0                         \n",
      "                  ab_norm: 110.0                         \n",
      "                 ab_quant: 10.0                          \n",
      "             aspect_ratio: 1.0                           \n",
      "           avg_loss_alpha: 0.986                         \n",
      "               batch_size: 8                             \n",
      "                    beta1: 0.9                           \n",
      "          checkpoints_dir: ./colorization_pytorch/checkpoints\n",
      "           classification: False                         \n",
      "             dataset_mode: aligned                       \n",
      "             display_freq: 200                           \n",
      "               display_id: 1                             \n",
      "            display_ncols: 5                             \n",
      "             display_port: 8097                          \n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 384                           \n",
      "              epoch_count: 0                             \n",
      "                 fineSize: 384                           \n",
      "                  gpu_ids: 0                             \n",
      "                     half: False                         \n",
      "                 how_many: 200                           \n",
      "                init_type: normal                        \n",
      "                 input_nc: 1                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                   l_cent: 50.0                          \n",
      "                   l_norm: 100.0                         \n",
      "                 lambda_A: 1.0                           \n",
      "                 lambda_B: 1.0                           \n",
      "               lambda_GAN: 0.0                           \n",
      "          lambda_identity: 0.5                           \n",
      "                 loadSize: 384                           \n",
      "               load_model: True                          \n",
      "                       lr: 0.0001                        \n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: lambda                        \n",
      "                mask_cent: 0.5                           \n",
      "         max_dataset_size: inf                           \n",
      "                    model: pix2pix                       \n",
      "                 nThreads: 4                             \n",
      "               n_layers_D: 3                             \n",
      "                     name: experiment_name               \n",
      "                      ndf: 64                            \n",
      "                      ngf: 64                            \n",
      "                    niter: 100                           \n",
      "              niter_decay: 100                           \n",
      "               no_dropout: False                         \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                 no_lsgan: False                         \n",
      "                     norm: batch                         \n",
      "                output_nc: 2                             \n",
      "                    phase: val                           \n",
      "                pool_size: 50                            \n",
      "               print_freq: 200                           \n",
      "           resize_or_crop: resize_and_crop               \n",
      "              results_dir: ./results/                    \n",
      "                sample_Ps: [1, 2, 3, 4, 5, 6, 7, 8, 9]   \n",
      "                 sample_p: 1.0                           \n",
      "          save_epoch_freq: 1                             \n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 10000                         \n",
      "                  verbose: False                         \n",
      "          which_direction: AtoB                          \n",
      "              which_epoch: latest                        \n",
      "         which_model_netD: basic                         \n",
      "         which_model_netG: siggraph                      \n",
      "----------------- End -------------------\n",
      "1\n",
      "1\n",
      "siggraph\n",
      "initialize network with normal\n",
      "<bound method Module.type of DataParallel(\n",
      "  (module): SIGGRAPHGenerator(\n",
      "    (model1): Sequential(\n",
      "      (0): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace)\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (model2): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace)\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (model3): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace)\n",
      "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU(inplace)\n",
      "      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (model4): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace)\n",
      "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU(inplace)\n",
      "      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (model5): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
      "      (3): ReLU(inplace)\n",
      "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
      "      (5): ReLU(inplace)\n",
      "      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (model6): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
      "      (3): ReLU(inplace)\n",
      "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
      "      (5): ReLU(inplace)\n",
      "      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (model7): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace)\n",
      "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU(inplace)\n",
      "      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (model8up): Sequential(\n",
      "      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (model8): Sequential(\n",
      "      (0): ReLU(inplace)\n",
      "      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): ReLU(inplace)\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): ReLU(inplace)\n",
      "      (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (model9up): Sequential(\n",
      "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (model9): Sequential(\n",
      "      (0): ReLU(inplace)\n",
      "      (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): ReLU(inplace)\n",
      "      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (model10up): Sequential(\n",
      "      (0): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (model10): Sequential(\n",
      "      (0): ReLU(inplace)\n",
      "      (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (model3short8): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (model2short9): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (model1short10): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (model_class): Sequential(\n",
      "      (0): Conv2d(256, 529, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (model_out): Sequential(\n",
      "      (0): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (upsample4): Sequential(\n",
      "      (0): Upsample(scale_factor=4, mode=nearest)\n",
      "    )\n",
      "    (softmax): Sequential(\n",
      "      (0): Softmax()\n",
      "    )\n",
      "  )\n",
      ")>\n",
      "model [Pix2PixModel] was created\n",
      "loading the model from ./colorization_pytorch/checkpoints/experiment_name/latest_net_G.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 34.187 M\n",
      "-----------------------------------------------\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 34.187 M\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"colorization_pytorch\")\n",
    "sys.path.append(\"flownet2_pytorch\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from colorization_pytorch.models import create_model\n",
    "from flownet2_pytorch.models import FlowNet2\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from datasets import *\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from colorization_pytorch.options.train_options import TrainOptions\n",
    "from flownet2_pytorch.parser import initialize_args\n",
    "import torchvision.transforms as transforms\n",
    "from colorization_pytorch.util import util\n",
    "\n",
    "class Mask(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Mask,self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(in_channels=8, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        return self.cnn(data)\n",
    "\n",
    "\n",
    "class VideoColorization(nn.Module):\n",
    "    def __init__(self, opt, args, batchNorm=False, div_flow=20.):\n",
    "        super(VideoColorization,self).__init__()\n",
    "        IDX_RANGE = 44\n",
    "        self.opt = opt\n",
    "        self.color_model = create_model(opt)\n",
    "        self.color_model.setup(opt)\n",
    "        self.color_model.print_networks(False)\n",
    "        self.flownet = FlowNet2(args, batchNorm=batchNorm, div_flow=div_flow)\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        self.flownet.load_state_dict(checkpoint['state_dict'])\n",
    "        self.ori_index = torch.tensor(list(itertools.product(np.arange(IDX_RANGE), np.arange(IDX_RANGE)))). \\\n",
    "            reshape(IDX_RANGE, IDX_RANGE, -1)\n",
    "        self.mask = Mask(512)\n",
    "\n",
    "    def forward(self, color_data, flow_data):\n",
    "        self.color_model.set_input(color_data[0])\n",
    "        _,_, previous_feature_map = self.color_model.encode()\n",
    "        print(\"color1\")\n",
    "        self.color_model.set_input(color_data[1])\n",
    "        conv1_2, conv2_2, feature_map = self.color_model.encode()\n",
    "        print(\"color2\")\n",
    "        flow = self.flownet(flow_data)  # B * 2 * 384 * 1024\n",
    "        print(\"flow\")\n",
    "        FlowWeight = F.interpolate(flow, previous_feature_map.size()[-2:], mode='bilinear')\n",
    "        H1, W1 = previous_feature_map.size()[-2:]\n",
    "        FlowWeight = FlowWeight.permute(0, 2, 3, 1).int()\n",
    "        FlowWeight += self.ori_index\n",
    "        FlowWeight[FlowWeight < 0] = 0\n",
    "        FlowWeight[FlowWeight >= H1] = H1 - 1\n",
    "        predicted_feature_map = previous_feature_map[FlowWeight]  # B*512*22*22\n",
    "        delta_feature_map = torch.abs(predicted_feature_map - feature_map)\n",
    "        M = self.mask(delta_feature_map)\n",
    "        output_feature_map = (1 - M) * feature_map + M * predicted_feature_map\n",
    "        fake_B_class, fake_B_reg = self.color_model.decode(conv1_2, conv2_2, output_feature_map)\n",
    "        \n",
    "\n",
    "        \n",
    "class flow_args():\n",
    "    def __init__(self):\n",
    "        self.rgb_max = 255\n",
    "        self.fp16 = False\n",
    "        self.fp16_scale = 1024.\n",
    "        self.crop_size = [384,1024]\n",
    "        self.inference_size = [-1,-1]\n",
    "        self.resume = './check_point/FlowNet2_checkpoint.pth.tar'\n",
    "        \n",
    "\n",
    "opt = TrainOptions().parse()\n",
    "args = flow_args()\n",
    "debug = VideoColorization(opt, args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MpiSintel(args, root = \"data/training/\")\n",
    "train_loader = DataLoader(train_set, batch_size = 2, shuffle=True, num_workers=4)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class solver():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.lr = 1e-4\n",
    "        self.optimizer_flow = torch.optim.Adam(self.model.flownet.parameters(), lr=2e-4, weight_decay=1e-6)\n",
    "        self.optimizer_color_network = []\n",
    "        self.use_D = opt.lambda_GAN > 0\n",
    "        self.optimizer_G = torch.optim.Adam(self.model.color_model.netG.parameters(),\n",
    "                                            lr=0, betas=(opt.beta1, 0.999)) #lr=opt.lr\n",
    "        self.optimizer_color_network.append(self.optimizer_G)\n",
    "\n",
    "        if self.use_D:\n",
    "            self.optimizer_D = torch.optim.Adam(self.model.color_model.netD.parameters(),\n",
    "                                                    lr=0, betas=(opt.beta1, 0.999))\n",
    "            self.optimizer_color_network.append(self.optimizer_D)\n",
    "        \n",
    "        self.optimizer_mask = torch.optim.Adam(self.model.mask.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "        self.batch_size = 5\n",
    "        self.epoch = 100\n",
    "        self.H = 384\n",
    "        self.W = 1024\n",
    "        self.ori_index = torch.tensor(list(itertools.product(np.arange(self.H), np.arange(self.W)))). \\\n",
    "            reshape(self.H, self.W, -1)\n",
    "        \n",
    "    def plot_grad_flow(self, named_parameters):\n",
    "        '''Plots the gradients flowing through different layers in the net during training.\n",
    "        Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "    \n",
    "        Usage: Plug this function in Trainer class after loss.backwards() as \n",
    "        \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
    "        ave_grads = []\n",
    "        max_grads= []\n",
    "        layers = []\n",
    "        for n, p in named_parameters:\n",
    "            if(p.requires_grad) and (\"bias\" not in n):\n",
    "                layers.append(n)\n",
    "                ave_grads.append(p.grad.abs().mean())\n",
    "                max_grads.append(p.grad.abs().max())\n",
    "        plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "        plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "        plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "        plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "        plt.xlim(left=0, right=len(ave_grads))\n",
    "        plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
    "        plt.xlabel(\"Layers\")\n",
    "        plt.ylabel(\"average gradient\")\n",
    "        plt.title(\"Gradient flow\")\n",
    "        plt.grid(True)\n",
    "        plt.legend([matplotlib.lines.Line2D([0], [0], color=\"c\", lw=4),\n",
    "                matplotlib.lines.Line2D([0], [0], color=\"b\", lw=4),\n",
    "                matplotlib.lines.Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n",
    "        plt.show()\n",
    "        \n",
    "    def pixel_flow(self, flow, img):\n",
    "        FlowWeight = flow.int()\n",
    "        FlowWeight += self.ori_index\n",
    "        FlowWeight[FlowWeight < 0] = 0\n",
    "        FlowWeight[FlowWeight >= self.H] = self.H - 1\n",
    "        out = img[FlowWeight]\n",
    "        return out\n",
    "        \n",
    "    def eval_metric(self, O_1, O_2, flow, mask):\n",
    "        R_2 = self.pixel_flow(flow, O_1)\n",
    "        Estab = torch.sum(mask * (O_2 - R_2) * (O_2 - R_2))\n",
    "        return Estab\n",
    "    \n",
    "    def cohe_loss(self, O_2, S_1, flow, mask):\n",
    "        R_2 = self.pixel_flow(flow, S_1)\n",
    "        L_cohe = torch.sum(mask * (O_2 - R_2) * (O_2 - R_2))\n",
    "        return L_cohe\n",
    "    \n",
    "    def occ_loss(self, O_2, S_2, mask):\n",
    "        L_occ = torch.sum((1-mask) * (O_2 - S_2) * (O_2 - S_2))\n",
    "        return L_occ\n",
    "        \n",
    "    def flow_loss(self, F_est, flow):\n",
    "        F_down = F.interpolate(flow, F_est.size(), mode='bilinear')\n",
    "        L_flow = torch.sum((F_est-F_down) * (F_est-F_down))\n",
    "        return L_flow\n",
    "        \n",
    "    def train(self, train_loader):\n",
    "        alpha = 1e-5\n",
    "        beta = 2e-4\n",
    "        gamma = 20\n",
    "        for epoch in range(1, self.epoch):\n",
    "            for Img, flow, mask in train_loader:\n",
    "                Img = Img.cuda()\n",
    "                print(Img.shape)\n",
    "                temp_1 = util.get_colorization_data(Img[:,:,0,:,:].unsqueeze(0),self.model.opt)\n",
    "                I_1 = temp_1['A']\n",
    "                S_1 = temp_1['B']\n",
    "                temp_2 = util.get_colorization_data(Img[:,:,1,:,:].unsqueeze(0),self.model.opt)\n",
    "                I_2 = temp_2['A']\n",
    "                S_2 = temp_2['B']\n",
    "                inp = torch.cat((I_1,I_2), dim=1)\n",
    "                self.optimizer_flow.zero_grad()\n",
    "                self.optimizer_G.zero_grad()\n",
    "                self.optimizer_mask.zero_grad()\n",
    "                _, O_2, F_est = self.model([temp_1,temp_2],inp)\n",
    "                print(2)\n",
    "                loss = alpha*self.cohe_loss(O_2,S_1,flow,mask)+beta*self.occ_loss(O_2,S_2,mask)+gamma*self.flow_loss(F_est,flow)\n",
    "                loss.backward()\n",
    "                self.optimizer_flow.step()\n",
    "                self.optimizer_G.step()\n",
    "                self.optimizer_mask.step()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = debug.cuda()\n",
    "solv = solver(debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2, 384, 1024])\n",
      "color1\n",
      "color2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 7, 7], expected input[2, 1, 384, 1024] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a81e0af91f26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msolv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-63a3b5fbfe50>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF_est\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemp_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemp_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcohe_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mO_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mS_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mocc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mO_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mS_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF_est\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-b52f641b2cfa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, color_data, flow_data)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mconv1_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv2_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"color2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflownet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflow_data\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# B * 2 * 384 * 1024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"flow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mFlowWeight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_feature_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/automatic_colorization/flownet2_pytorch/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# flownetc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mflownetc_flow2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflownetc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mflownetc_flow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflownetc_flow2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_flow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/automatic_colorization/flownet2_pytorch/networks/FlowNetC.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# FlownetC bottom input stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mout_conv1b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mout_conv2b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_conv1b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[2, 1, 384, 1024] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "solv.train(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3,4).unsqueeze(1)\n",
    "b = torch.ones(3,4).unsqueeze(1)\n",
    "torch.cat((a,b), dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
